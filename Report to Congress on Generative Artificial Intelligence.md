---
title: "Report to Congress on Generative Artificial Intelligence"
source: "https://news.usni.org/2025/04/08/report-to-congress-on-generative-artificial-intelligence"
author:
  - "[[U.S. Naval Institute Staff]]"
published: 2025-04-08
created: 2025-04-27
description: "The following is the April 2, 2025, Congressional Research Service In Focus report, Generative Artificial Intelligence: Overview, Issues, and Considerations for Congress. From the report Generative artificial intelligence (GenAI) refers to AI models, in particular those that use machine learning (ML) and are trained on large volumes of data, that are able to generate new content. In contrast, other AI models may have a primary goal of classifying data, such as facial recognition image data, or making decisions, such as those used in automated vehicles. GenAI, when prompted (often by a user inputting text), can create various outputs, including text,"
tags:
  - "clippings"
---
[![](https://news.usni.org/wp-content/uploads/2016/02/usni_logo.png)](https://news.usni.org/)

*The following is the April 2, 2025, Congressional Research Service In Focus report, Generative Artificial Intelligence: Overview, Issues, and Considerations for Congress.*

## From the report

Generative artificial intelligence (GenAI) refers to AI models, in particular those that use machine learning (ML) and are trained on large volumes of data, that are able to generate new content. In contrast, other AI models may have a primary goal of classifying data, such as facial recognition image data, or making decisions, such as those used in automated vehicles. GenAI, when prompted (often by a user inputting text), can create various outputs, including text, images, videos, computer code, or music.

The public release of many GenAI tools, and the race by companies to develop ever-more powerful AI models, have generated widespread discussion of their capabilities, potential concerns with their use, and debates about their governance and regulation. This CRS In Focus describes the development and uses of GenAI, concerns raised by the use of GenAI tools, and considerations for Congress.

Background

AI can generally be thought of as computerized systems that work and react in ways commonly considered to require human intelligence, such as learning, solving problems, and achieving goals under uncertain and varying conditions, with varying levels of autonomy. AI can encompass a range of technologies, methodologies, and application areas, such as natural language processing, robotics, and facial recognition.

The AI technologies underpinning many GenAI tools are the result of decades of research. For example, recurrent neural networks (RNNs), a type of ML loosely modeled after the human brain that detects patterns in sequential data, underwent much development and improvement in the 1980s-1990s. RNNs can generate text, but they have limited ability to retain contextual information across large strings of words, are slow to train, and are not easily scaled up by increasing computational power or training data size.

More recent technical advances—notably the introduction of the Transformer architecture by Google researchers in 2017 and improvements in generative pre-trained transformer (GPT) models since around 2019—have contributed to dramatic improvement in GenAI performance. Transformer models process a sequence of whole sentences rather than analyzing word by word. They use mathematical techniques called attention or self-attention to detect how data elements, even when far away sequentially, influence and depend on each other. These methods make GPT models faster to train, more efficient in understanding context, and highly scalable.

Other critical components to recent GenAI advances have been the availability of large amounts of data and the size of their language models. Large language models (LLMs) are AI systems that aim to model language, sometimes using millions or billions of parameters (i.e., numbers in the model that determine how inputs are converted to outputs). Repeatedly tweaking these parameters, using mathematical optimization techniques and large amounts of data and computational power, increases model performance. Notably, GenAI models work to match the style and appearance of the underlying training data. They have also demonstrated emergent abilities, meaning capabilities that their developers and users did not anticipate but that are emerging as the models grow larger.

LLMs have been characterized as foundation models (also called general-purpose AI), meaning models trained on large and diverse datasets that can be adapted to a wide range of downstream tasks. As described by the Stanford University Institute for Human-Centered AI, foundation models may be built on or integrated into multiple AI systems across various domains (e.g., text-based GPT models that can perform arithmetic and computer programming tasks, which were outside the scope of their original training). This capability has the potential for both benefits (e.g., concentrating efforts to reduce bias and improve robustness) and drawbacks (e.g., security failures or inequities that flow to downstream applications).

Download the document [here](https://www.documentcloud.org/documents/25887178-generative-artificial-intelligence-overview-issues-and-considerations-for-congress-april-2-2025/).